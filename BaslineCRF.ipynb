{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Conditional Random Fields for Surface Segmentation Feasibility Demo 18/08/2020\n",
    "## Tumi Moeng"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning\n",
    "First, the data we received needs to be cleaned and the surface segmentation\n",
    " forms need to be generated to be used by the models. To this end I created a\n",
    " class called 'DataCleaner' which takes the data we received and filters out\n",
    " that which we need from that which we dont need and generates some more data\n",
    " that we need.<br>\n",
    " This class takes the data in the following form:<br>\n",
    " ngezinkonzo&emsp;khonzo&emsp;P&emsp;[RelConc]-nga[NPre]-i[NPrePre]-zin[BPre]-konzo[NStem]<br>\n",
    " And converts it to the following form:<br>\n",
    " ngezinkonzo | nge-zin-konzo | nga[NPre]i[NPrePre]zin[BPre]konzo[NStem]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from morphology.DataCleaner import DataCleaner\n",
    "languages = [\"zulu\", \"swati\", \"ndebele\", \"xhosa\"]\n",
    "\n",
    "for lang in languages:\n",
    "    print(\"Language: \" + lang)\n",
    "    inputFile = DataCleaner(\"morphology\\\\\"+lang + \".train.conll\")\n",
    "    inputFile.reformat(\"morphology\\\\\"+lang + \".clean.train\")\n",
    "    inputFile = DataCleaner(\"morphology\\\\\"+lang + \".test.conll\")\n",
    "    inputFile.reformat(\"morphology\\\\\"+lang + \".clean.test\")\n",
    "    print(lang + \" cleaning complete.\\n#############################################\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: zulu\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'zulu.train.conll'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-4b313527d7ab>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mmorphology\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataCleaner\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mDataCleaner\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mlanguages\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;34m\"zulu\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"swati\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"ndebele\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"xhosa\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mlang\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mlanguages\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Language: \"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mlang\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\UniWork\\Fourth Year\\CS Honours\\Honours Project\\CRFModels\\morphology\\DataCleaner.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    423\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mlang\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mlanguages\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    424\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Language: \"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mlang\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 425\u001B[1;33m     \u001B[0minputFile\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDataCleaner\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlang\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\".train.conll\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    426\u001B[0m     \u001B[0minputFile\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlang\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\".clean.train\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    427\u001B[0m     \u001B[0minputFile\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDataCleaner\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlang\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\".test.conll\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\UniWork\\Fourth Year\\CS Honours\\Honours Project\\CRFModels\\morphology\\DataCleaner.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, filename)\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfilename\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m         \u001B[1;31m# Open file for reading only\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfile\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"r\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlines\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfile\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreadlines\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0ms\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSymbols\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'zulu.train.conll'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparation for Hyper-parameter Optimisation\n",
    "Secondly, in preparation for the hyper-parameter optimisation that we will need to do\n",
    " to ensure our models are as good as possible we needed to be able to develop a test /\n",
    " validation set to be used in the optimisation process. To this end, I created a class\n",
    " called 'ValidationSetCreation' that extracts about 10% of the data from the training set\n",
    " and puts it into a validation set. This 10% represents roughly the same amount of entries\n",
    " as the test set contains."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: zulu\n",
      "3233\n",
      "zulu validation set complete.\n",
      "#############################################\n",
      "Language: swati\n",
      "5159\n",
      "swati validation set complete.\n",
      "#############################################\n",
      "Language: ndebele\n",
      "2985\n",
      "ndebele validation set complete.\n",
      "#############################################\n",
      "Language: xhosa\n",
      "3262\n",
      "xhosa validation set complete.\n",
      "#############################################\n"
     ]
    }
   ],
   "source": [
    "from morphology.ValidationSetCreation import ValidationSet\n",
    "\n",
    "for lang in languages:\n",
    "    print(\"Language: \" + lang)\n",
    "    file_name =\"morphology\\\\\" + lang + \".clean.train.conll\"\n",
    "    # print(file_name)\n",
    "    inputFile = ValidationSet(file_name)\n",
    "    file_name = \"morphology\\\\\"+lang + \".clean.dev.conll\"\n",
    "    # print(file_name)\n",
    "    inputFile.create_validation_set(file_name)\n",
    "    print(lang + \" validation set complete.\\n#############################################\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conditional Random Field using SKLearn_CRFSuite\n",
    "Finally, we are now able to train the CRF using the data we have developed and\n",
    "train the CRF on this data. We give it input as the string of the language it\n",
    "is to be trained on and it will give 2 outputs. The first is a list of lists of\n",
    "what the CRF predicted the labels will be for the words in the test set. The second\n",
    " is a list of lists of what the actual labels for the word in the test set are. Both\n",
    " inner lists of labels occur in the following form:<br>\n",
    " For the word 'komthombo' which segments to 'ko-m-thombo the inner list would be\n",
    " [B,E,S,B,M,M,M,M,E] where:<br>\n",
    " B = Beginning of Segment<br>\n",
    " M = Middle of Segment<br>\n",
    " E = End of Segment<br>\n",
    " S = Single Length Segment<br>\n",
    " The results method which takes in the list of predicted answers and actual answers\n",
    " can be used to print to console the results of the CRF such as the <b>precision</b>,\n",
    " <b>recall</b> and <b>F1-Score</b>."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Collected in -0.7810447000001659\n",
      "Beginning Feature Computation and Model Optimisation\n",
      "Features Successfully Computed & Model Optimised -0.0009993000003305497\n",
      "First Value of Predicted Answer List: ['B', 'M', 'M', 'M', 'E']\n",
      "First Value of Actual Answer List: ['B', 'M', 'M', 'M', 'E']\n",
      "\n",
      "Evaluation on the Test set\n",
      "\n",
      "delta = 8\tepsilon = 1e-07\tmax_iter = 80\tBest Algo = ap\n",
      "Precision = 0.8805540063208774\n",
      "Recall = 0.808621425522834\n",
      "F1-score = 0.8430561117785786\n",
      "0.881\t0.809\t0.843\n"
     ]
    }
   ],
   "source": [
    "import sklearn_crfsuite\n",
    "\n",
    "from BaselineCRF import BaselineCRF\n",
    "crf = BaselineCRF(\"zulu\")\n",
    "predicted_ans, actual_ans = crf.surface_segmentation()\n",
    "print(\"First Value of Predicted Answer List: \"+str(predicted_ans[0]))\n",
    "print(\"First Value of Actual Answer List: \"+str(actual_ans[0]))\n",
    "crf.results(predicted_ans, actual_ans)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}